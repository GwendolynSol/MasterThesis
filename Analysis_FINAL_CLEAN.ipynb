{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b22fed5",
   "metadata": {},
   "source": [
    "## 1. Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6fa9d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "folder_name = '' ## Enter the name of the folder where you want to save the output files\n",
    "# Output directory setup\n",
    "def setup_output_folder():\n",
    "    folder = folder_name\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "    return folder\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfafbcc2",
   "metadata": {},
   "source": [
    "## 2. Generate Datasets: Supplier X & Supplier Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55dc761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reverse_engineer_suppliers_from_company(company_data, gamma_x, gamma_y, seed=42):\n",
    "    \"\"\"\n",
    "    Reverse-engineers Supplier X and Y lead times from fixed company data.\n",
    "    Ensures that: X < Y < Company.\n",
    "    \"\"\"\n",
    "    n = len(company_data)\n",
    "    product_ids = company_data[\"ProductID\"].values\n",
    "    company_lead_times = company_data[\"Lead_Time\"].values\n",
    "    \n",
    "    # Generate random production and internal delays\n",
    "    np.random.seed(seed)\n",
    "    supplier_y_max = []\n",
    "    while len(supplier_y_max) < n:\n",
    "        delay = np.random.gamma(shape = 2, scale = 1.5)\n",
    "        if delay > 0 and delay <= company_lead_times[len(supplier_y_max)] - 3:\n",
    "            supplier_y_max.append(company_lead_times[len(supplier_y_max)] - delay)\n",
    "\n",
    "    # Randomly generate lead times for Supplier Y from a gamma distribution such that X < Y < Company \n",
    "    np.random.seed(seed + 2)\n",
    "    supplier_y_lead = []\n",
    "    while len(supplier_y_lead) < n:\n",
    "        y_lead = np.random.gamma(shape = gamma_y[0], scale = gamma_y[1])\n",
    "        if y_lead > 2 and y_lead < supplier_y_max[len(supplier_y_lead)]:\n",
    "            supplier_y_lead.append(y_lead)\n",
    "\n",
    "    # Randomly generate lead times for Supplier X from a gamma distribution such that X < Y < Company\n",
    "    np.random.seed(seed + 3)\n",
    "    supplier_x_lead = []\n",
    "    while len(supplier_x_lead) < n:\n",
    "        x_lead = np.random.gamma(shape = gamma_x[0], scale = gamma_x[1])\n",
    "        if x_lead > 1 and x_lead < supplier_y_lead[len(supplier_x_lead)]:\n",
    "            supplier_x_lead.append(x_lead)\n",
    "\n",
    "    # Create DataFrames\n",
    "    supplier_x_data = pd.DataFrame({'ProductID': product_ids, 'Lead_Time': supplier_x_lead})\n",
    "    supplier_y_data = pd.DataFrame({'ProductID': product_ids, 'Lead_Time': supplier_y_lead})\n",
    "\n",
    "    return supplier_x_data, supplier_y_data, company_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e35cc4",
   "metadata": {},
   "source": [
    "## 3. Correlation Analysis Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "506c8330",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_lead_time_correlations(company_data, supplier_x_data, supplier_y_data, situation=None, output_dir=\"DP_Results\", label=\"base\", title_info=\"\"):\n",
    "\n",
    "    if situation is None: # base\n",
    "        column_company = \"Lead_Time\"\n",
    "        company_other = \"Lead_Time\"\n",
    "    elif situation == \"Situation1\":\n",
    "        column_company = \"Noisy_Lead_Time\"\n",
    "        company_other = \"Lead_Time\"\n",
    "    elif situation == \"Situation2\":\n",
    "        column_company = \"Noisy_Lead_Time\"\n",
    "        company_other = \"Noisy_Lead_Time\"\n",
    "    else:\n",
    "        raise ValueError(\"Invalid situation. Choose from: None, 'Situation1', 'Situation2'.\")\n",
    "\n",
    "    # column = \"Noisy_Lead_Time\" if noisy else \"Lead_Time\"\n",
    "\n",
    "    min_length = min(len(company_data), len(supplier_x_data), len(supplier_y_data))\n",
    "    company_data = company_data.iloc[:min_length].reset_index(drop=True)\n",
    "    supplier_x_data = supplier_x_data.iloc[:min_length].reset_index(drop=True)\n",
    "    supplier_y_data = supplier_y_data.iloc[:min_length].reset_index(drop=True)\n",
    "\n",
    "    c = company_data[column_company]\n",
    "    x = supplier_x_data[company_other]\n",
    "    y = supplier_y_data[company_other]\n",
    "\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    sns.scatterplot(x=x, y=y, ax=axes[0], alpha=0.5)\n",
    "    sns.scatterplot(x=y, y=c, ax=axes[1], alpha=0.5)\n",
    "    sns.scatterplot(x=x, y=c, ax=axes[2], alpha=0.5)\n",
    "    axes[0].set_title(\"Supplier X vs Supplier Y\")\n",
    "    axes[1].set_title(\"Supplier Y vs Company\")\n",
    "    axes[2].set_title(\"Supplier X vs Company\")\n",
    "\n",
    "    filename = f\"{output_dir}/{title_info}_Correlations_Scatter_{label}.png\"\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.close()\n",
    "\n",
    "    return {\n",
    "        \"Supplier X ↔ Supplier Y (Spearman)\": spearmanr(x, y)[0],\n",
    "        \"Supplier Y ↔ Company (Spearman)\": spearmanr(y, c)[0],\n",
    "        \"Supplier X ↔ Company (Spearman)\": spearmanr(x, c)[0]\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f920d182",
   "metadata": {},
   "source": [
    "## 4. Sensitivity Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d30c8a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sensitivities(company_df, output_dir=\"DP_Results\"):\n",
    "    mean_lead_time = company_df[\"Lead_Time\"].mean()\n",
    "    company_df[\"Deviation\"] = abs(company_df[\"Lead_Time\"] - mean_lead_time)\n",
    "    max_deviation_row = company_df.loc[company_df[\"Deviation\"].idxmax()]\n",
    "    company_df_dprime = company_df.drop(index=max_deviation_row.name).drop(columns=[\"Deviation\"])\n",
    "\n",
    "    d = len(company_df)\n",
    "    d_prime = len(company_df_dprime)\n",
    "    delta_f_count = abs(d - d_prime)\n",
    "\n",
    "    mean_d = company_df[\"Lead_Time\"].mean()\n",
    "    mean_dprime = company_df_dprime[\"Lead_Time\"].mean()\n",
    "    delta_f_mean = abs(mean_d - mean_dprime)\n",
    "\n",
    "    company_df_dprime.to_csv(f\"{output_dir}/Dataset_1c_Company_Dprime.csv\", index=False)\n",
    "\n",
    "    return delta_f_count, delta_f_mean, mean_d, mean_dprime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f3a5e31",
   "metadata": {},
   "source": [
    "## 5. Apply Differential Privacy (Count and Mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "14ccf0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_dp_noise(df, sensitivity, epsilon, output_path):\n",
    "    b = sensitivity / epsilon\n",
    "    noise = np.random.laplace(loc=0, scale=b, size=len(df))\n",
    "    noisy_df = df.copy()\n",
    "    noisy_df[\"Noisy_Lead_Time\"] = noisy_df[\"Lead_Time\"] + noise\n",
    "    noisy_df.to_csv(output_path, index=False)\n",
    "    return noisy_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f19f61",
   "metadata": {},
   "source": [
    "## 6. MAE Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff19f7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mae_from_lists(baseline_list, private_list):\n",
    "    return np.mean([abs(b - p) for b, p in zip(baseline_list, private_list)])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d628c44",
   "metadata": {},
   "source": [
    "## 7. Histogram Error Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5e721278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_histogram(df, title, x_label, filename, output_dir):\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.hist(df, bins=30, edgecolor='black', color='skyblue')\n",
    "    plt.title(title)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(\"Frequency\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{output_dir}/{filename}.png\")\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b0b5e0",
   "metadata": {},
   "source": [
    "## 8. Main Function & Hyperparameter Tuning (Grid Search)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4115c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    output_dir = setup_output_folder()\n",
    "\n",
    "    epsilons = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0, 1.1, 1.2, 1.3, 1.4, 1.5, 1.6, 1.7, 1.8, 1.9, 2.0, 2.1, 2.2, 2.3, 2.4, 2.5, 2.6, 2.7, 2.8, 2.9, 3.0, 3.1, 3.2, 3.3, 3.4, 3.5, 3.6, 3.7, 3.8, 3.9, 4.0, 4.1, 4.2, 4.3, 4.4, 4.5, 4.6, 4.7, 4.8, 4.9, 5.0]\n",
    "    sample_sizes = [7706]\n",
    "    gamma_params = [\n",
    "        {\"x\": (2, 3), \"y\": (2.5, 4)},\n",
    "\n",
    "  \n",
    "\n",
    "    ]\n",
    "    situations = [\"Situation1\", \"Situation2\"]\n",
    "\n",
    "    company_data_full = pd.read_csv(\"Company_Lead_Time_data.csv\")\n",
    "\n",
    "    for situation in situations:\n",
    "        \n",
    "        results = []\n",
    "\n",
    "        for dist_id, dist in enumerate(gamma_params):\n",
    "            for n in sample_sizes:\n",
    "                for eps in epsilons:\n",
    "                    title_info = f\"{situation}/ε{eps}_n{n}_x{dist['x']}_y{dist['y']}\".replace(\" \", \"\")\n",
    "\n",
    "                    # Step 1: Company sample\n",
    "                    company_sample = company_data_full.sample(n=n, random_state=42).reset_index(drop=True)\n",
    "\n",
    "                    # Step 2: Reverse engineer X and Y\n",
    "                    sx, sy, co = reverse_engineer_suppliers_from_company(company_sample, dist[\"x\"], dist[\"y\"])\n",
    "\n",
    "                    # Step 3: Save raw datasets\n",
    "                    sx.to_csv(f\"{output_dir}/{title_info}_Dataset_2_Supplier_X_with_dependencies.csv\", index=False)\n",
    "                    sy.to_csv(f\"{output_dir}/{title_info}_Dataset_3_Supplier_Y_with_dependencies.csv\", index=False)\n",
    "                    co.to_csv(f\"{output_dir}/{title_info}_Dataset_1_Company_with_dependencies.csv\", index=False)\n",
    "\n",
    "                    # Step 4: Plot histograms\n",
    "                    plot_histogram(sx[\"Lead_Time\"], \"Supplier X Lead Time\", \"Lead Time\", f\"{title_info}_hist_supplierX\", output_dir)\n",
    "                    plot_histogram(sy[\"Lead_Time\"], \"Supplier Y Lead Time\", \"Lead Time\", f\"{title_info}_hist_supplierY\", output_dir)\n",
    "                    plot_histogram(co[\"Lead_Time\"], \"Company Lead Time\", \"Lead Time\", f\"{title_info}_hist_company\", output_dir)\n",
    "\n",
    "                    # Step 4: Baseline correlations\n",
    "                    base_corrs = analyze_lead_time_correlations(co, sx, sy, output_dir=output_dir, label=\"baseline\", title_info=title_info)\n",
    "                    \n",
    "                    # Step 5: Sensitivities\n",
    "                    delta_count, delta_mean, _, _ = calculate_sensitivities(co, output_dir)\n",
    "\n",
    "                    # Step 6: Apply DP to all or the company only (depending on the situation)\n",
    "                    if situation == \"Situation1\": # Situation 1\n",
    "                        d4_co_mean = apply_dp_noise(co, delta_mean, eps, f\"{output_dir}/{title_info}_Dataset_4_Company_DP_MeanLeadTimeQuery.csv\")\n",
    "                        d4_co_count = apply_dp_noise(co, delta_count, eps, f\"{output_dir}/{title_info}_Dataset_4_Company_DP_CountQuery.csv\")\n",
    "                      \n",
    "                        # Step 7: Correlations with DP data\n",
    "                        dp_corr_mean = analyze_lead_time_correlations(d4_co_mean, sx, sy, situation=\"Situation1\", output_dir=output_dir, label=\"dp_mean\", title_info=title_info)\n",
    "\n",
    "                    else: # Situation 2\n",
    "                        d4_co_mean = apply_dp_noise(co, delta_mean, eps, f\"{output_dir}/{title_info}_Dataset_4_Company_DP_MeanLeadTimeQuery.csv\")\n",
    "                        d4_co_count = apply_dp_noise(co, delta_count, eps, f\"{output_dir}/{title_info}_Dataset_4_Company_DP_CountQuery.csv\")\n",
    "                        d4_sx = apply_dp_noise(sx, delta_mean, eps, f\"{output_dir}/{title_info}_Dataset_4_Supplier_X_DP.csv\")\n",
    "                        d4_sy = apply_dp_noise(sy, delta_mean, eps, f\"{output_dir}/{title_info}_Dataset_4_Supplier_Y_DP.csv\")\n",
    "\n",
    "                        # Step 7: Correlations with DP data\n",
    "                        dp_corr_mean = analyze_lead_time_correlations(d4_co_mean, d4_sx, d4_sy, situation=\"Situation2\", output_dir=output_dir, label=\"dp_mean\", title_info=title_info)\n",
    "\n",
    "                    # Step 8: MAE per relationship\n",
    "                    mae_x_y = abs(base_corrs[\"Supplier X ↔ Supplier Y (Spearman)\"] - dp_corr_mean[\"Supplier X ↔ Supplier Y (Spearman)\"])\n",
    "                    mae_y_c = abs(base_corrs[\"Supplier Y ↔ Company (Spearman)\"] - dp_corr_mean[\"Supplier Y ↔ Company (Spearman)\"])\n",
    "                    mae_x_c = abs(base_corrs[\"Supplier X ↔ Company (Spearman)\"] - dp_corr_mean[\"Supplier X ↔ Company (Spearman)\"])\n",
    "\n",
    "                    # Step 9: Save histograms\n",
    "                    # plot_error_histogram(co, d4_co_mean, \"Error - Company Mean Query\", f\"{title_info}_hist_mean_company\", output_dir)\n",
    "                    plot_histogram(d4_co_mean[\"Noisy_Lead_Time\"] - co[\"Lead_Time\"], \"Company Lead Time with DP\", \"Lead Time Error (Noisy - True)\", f\"{title_info}_hist_dp_company\", output_dir)\n",
    "\n",
    "                    if situation == \"Situation 2\":\n",
    "                        plot_histogram(d4_sx[\"Noisy_Lead_Time\"] - sx[\"Lead_Time\"], \"Supplier X Lead Time with DP\", \"Lead Time Error (Noisy - True)\", f\"{title_info}_hist_dp_supplierX\", output_dir)\n",
    "                        plot_histogram(d4_sy[\"Noisy_Lead_Time\"] - sy[\"Lead_Time\"], \"Supplier Y Lead Time with DP\", \"Lead Time Error (Noisy - True)\", f\"{title_info}_hist_dp_supplierY\", output_dir)\n",
    "\n",
    "                        # plot_error_histogram(sx, d4_sx, \"Error - Supplier X\", f\"{title_info}_hist_dp_supplierX\", output_dir)\n",
    "                        # plot_error_histogram(sy, d4_sy, \"Error - Supplier Y\", f\"{title_info}_hist_dp_supplierY\", output_dir)\n",
    "\n",
    "                    # Step 10: Store result\n",
    "                    results.append({\n",
    "                        \"epsilon\": eps,\n",
    "                        \"sample_size\": n,\n",
    "                        \"gamma_x\": dist[\"x\"],\n",
    "                        \"gamma_y\": dist[\"y\"],\n",
    "                        \"MAE_XY\": mae_x_y,\n",
    "                        \"MAE_YC\": mae_y_c,\n",
    "                        \"MAE_XC\": mae_x_c,\n",
    "                    })\n",
    "\n",
    "        # Step 11: Save results\n",
    "        results_df = pd.DataFrame(results)\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        results_path = f\"{output_dir}/{situation}/Grid_Search_Results_{timestamp}.xlsx\"\n",
    "        results_df.to_excel(results_path, index=False)\n",
    "        print(f\" Grid search completed. Results saved to {results_path}\")\n",
    "\n",
    "        # Step 12: Plot efficient frontiers for each correlation\n",
    "        relationships = {\n",
    "            \"MAE_XY\": \"Supplier X ↔ Supplier Y\",\n",
    "            \"MAE_YC\": \"Supplier Y ↔ Company\",\n",
    "            \"MAE_XC\": \"Supplier X ↔ Company\"\n",
    "        }\n",
    "\n",
    "        for mae_key, label in relationships.items():\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            for n in sample_sizes:\n",
    "                subset = results_df[results_df[\"sample_size\"] == n]\n",
    "                plt.plot(subset[mae_key], subset[\"epsilon\"], marker='o', label=f\"Sample Size: {n}\")\n",
    "            plt.gca().invert_xaxis()\n",
    "            plt.gca().invert_yaxis()\n",
    "            plt.xlabel(\"Accuracy (Lower MAE → Better)\")\n",
    "            plt.ylabel(\"Privacy (Lower Epsilon → Better)\")\n",
    "            plt.title(f\"Efficient Frontier: {label} in {situation}\")\n",
    "            plt.legend()\n",
    "            plt.grid(True)\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"{output_dir}/{situation}/Efficient_Frontier_{mae_key}.png\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Run the main function\n",
    "main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
